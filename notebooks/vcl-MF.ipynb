{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: currently I only perform validations loops at the end of each epoch if early stopping is enabled.\n",
    "# NOTE: when transformed to python script magics and parse args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import argparse\n",
    "\n",
    "# parameters to tune on Eddie\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--init-scale\", default=\"1e-2\", help=\"guide factory initial parameter scale\")\n",
    "parser.add_argument(\n",
    "    \"--seed\", default=\"6202\", help=\"random seed for reproducibility\")\n",
    "# parser.add_argument(\"--inference\", choices=[\"mean-field\", \"ml\"], required=True)\n",
    "\n",
    "# parsed_args = parser.parse_args([\"--init-scale\", \"1e-3\", \"--seed\", \"3\"])\n",
    "parsed_args = parser.parse_args([])\n",
    "# parsed_args = parser.parse_args()\n",
    "parsed_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import functools\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# import torchvision.transforms as tf\n",
    "# from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import tyxe\n",
    "\n",
    "from MF.model import get_model\n",
    "from dataset.ASMGMovieLens import ASMGMovieLens\n",
    "from utils.save import get_version, save_as_json\n",
    "from pytorchtools import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tyxe global parameters\n",
    "ROOT = os.environ.get(\"DATASETS_PATH\", \"./data\")\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\") if USE_CUDA else torch.device(\"cpu\")\n",
    "C10_MEAN = (0.49139968, 0.48215841, 0.44653091)\n",
    "C10_SD = (0.24703223, 0.24348513, 0.26158784)\n",
    "inference = \"mean-field\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control flow parameters\n",
    "validation_config = True\n",
    "test_offline = False\n",
    "test_online = False\n",
    "plot_perf = False\n",
    "fast_dev_run = True\n",
    "\n",
    "train_params = dict(\n",
    "    input_path=\"data/movielens/processed/ml_processed.csv\",\n",
    "    val_start_period=11,\n",
    "    val_end_period= 24,  #12\n",
    "    test_start_period=25, # change to None if running as validation\n",
    "    test_end_period=31,  # 25,\n",
    "    train_window=10,\n",
    "    seed=int(parsed_args.seed),\n",
    "    model_filename='first_mf',\n",
    "    offline_path=None,#\"../model/MF/mean-field/version_29/offline_state_dict.pt\",\n",
    "    online_end_of_validation_path=None,  # \"../model/MF/mean-field/version_14/online_state_dict.pt\",\n",
    "    # save_model=False, \\todo\n",
    "    save_result=True\n",
    ")\n",
    "\n",
    "model_params = dict(\n",
    "    alias=\"MF\",\n",
    "    n_users=43183,\n",
    "    n_items=51149,\n",
    "    n_latents=8,\n",
    "    l2_regularization_constant=1e-6,\n",
    "    learning_rate=1e-3,  # 1e-2 is the ASMG MF implementation\n",
    "    batch_size=1024,\n",
    "    n_epochs_offline=30, # 11,\n",
    "    n_epochs_online=40,  # 19,\n",
    "    early_stopping_offline=True,\n",
    "    early_stopping_online=True, # train_params[\"test_start_period\"] is None,\n",
    "    update_prior=True,\n",
    "    random_init=True,\n",
    "    test_samples=40,\n",
    "    guide_init_scale=float(parsed_args.init_scale)\n",
    ")\n",
    "\n",
    "# train_batch_size = 1024\n",
    "# test_batch_size = 1000\n",
    "train_params[\"model_checkpoint_dir\"] = f'./../model/{model_params[\"alias\"]}'\n",
    "\n",
    "if fast_dev_run:\n",
    "    model_params[\"n_epochs_offline\"] = 1\n",
    "    model_params[\"n_epochs_online\"] = 1\n",
    "    model_params[\"save_result\"] = False\n",
    "\n",
    "if validation_config:\n",
    "    train_params.update(dict(\n",
    "        val_start_period=11,\n",
    "        val_end_period= 20,\n",
    "        test_start_period=21, \n",
    "        test_end_period=24, \n",
    "    ))\n",
    "\n",
    "\n",
    "# adapt function to TyXe experiment \n",
    "get_version = functools.partial(get_version, logdir=inference)\n",
    "experiment_params = {**train_params, **model_params}\n",
    "params = argparse.Namespace(**experiment_params)\n",
    "params.guide_init_scale, params.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get version\n",
    "version = get_version(train_params[\"model_checkpoint_dir\"])\n",
    "\n",
    "# make checkpoint dir\n",
    "model_checkpoint_subdir = train_params[\"model_checkpoint_dir\"] + (\n",
    "    f'/{inference}/{version}')\n",
    "    \n",
    "if not os.path.exists(model_checkpoint_subdir):\n",
    "    os.makedirs(model_checkpoint_subdir)\n",
    "    \n",
    "    # save json \n",
    "    json_path = f\"{model_checkpoint_subdir}/params\"\n",
    "    save_as_json(experiment_params, json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# script functions\n",
    "def validation_loop(model, dataloader, model_samples):\n",
    "    \"\"\"Returns error and likelihood as cpu floats.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BNN\n",
    "    dataloader \n",
    "    model_samples : int\n",
    "        number of samples of model parameters for the evaluation.\n",
    "    \n",
    "    \"\"\"    \n",
    "    err, log_likelihood = torch.tensor([\n",
    "        model.evaluate(x.to(DEVICE), y.to(DEVICE), \n",
    "        num_predictions=model_samples\n",
    "            ) for x, y in dataloader]\n",
    "        ).sum(dim = 0)\n",
    "    mean_nll = - log_likelihood.item() / len(dataloader.sampler)\n",
    "    mean_error =  err.item() / len(dataloader.sampler)\n",
    "\n",
    "    return mean_nll, mean_error\n",
    "\n",
    "def fit_aux(train_loader, n_epochs, \n",
    "    early_stopper, **kwargs):\n",
    "\n",
    "    # # veryfy callback_type\n",
    "    # implemented_callbacks = (\"reporting\", \"early-stopping\")\n",
    "    # assert callback_type in implemented_callbacks, (\n",
    "    #     f\"`callback_type` must be in {implemented_callbacks}\")\n",
    "\n",
    "    elbos = []\n",
    "    postfix_dict = {\"Epoch Loss\": np.Inf}\n",
    "    pbar = tqdm(total=n_epochs, \n",
    "    unit=\"Epochs\", postfix=f\"Epoch Loss: {postfix_dict['Epoch Loss']}\")\n",
    "\n",
    "    # non-enclosed function\n",
    "    def reporting_callback(model, _ii, e):\n",
    "        \"\"\"\n",
    "        Used at the end of each epoch on the TyXe `fit` method.\n",
    "        \n",
    "        model: BNN\n",
    "        _ii: epoch number\n",
    "        e: epoch loss\n",
    "        \"\"\"\n",
    "        mean_loss = e / len(train_loader.sampler) / train_loader.batch_size\n",
    "        elbos.append(mean_loss)\n",
    "        pbar.update()\n",
    "        postfix_dict['Epoch Loss'] = f\"{mean_loss:.5f}\"\n",
    "        pbar.set_postfix(postfix_dict)\n",
    "\n",
    "    if early_stopper is not None:\n",
    "        test_errors = []\n",
    "        test_nll = []\n",
    "\n",
    "        def early_stopping_callback(model, _ii, e):\n",
    "            \"\"\"Calls `reporting_callback` and adds validation loop for \n",
    "            early-stopping.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            model: BNN\n",
    "            _ii: epoch number\n",
    "            e: epoch loss\n",
    "            \"\"\"           \n",
    "\n",
    "            reporting_callback(model, _ii, e)\n",
    "\n",
    "            # include this snippet in callback\n",
    "            mean_nll, mean_error =  validation_loop(\n",
    "                model, kwargs[\"test_dataloader\"], kwargs[\"test_samples\"])\n",
    "            test_errors.append(mean_error)\n",
    "            test_nll.append(mean_nll)\n",
    "            \n",
    "            # update pbar with validation results\n",
    "            postfix_dict[\"Val Loss\"] = f\"{mean_nll:.5f}\"\n",
    "            pbar.set_postfix(postfix_dict)\n",
    "\n",
    "            early_stopper(mean_nll, model)\n",
    "\n",
    "            if early_stopper.early_stop:\n",
    "                print(f\"early-stopped: {early_stopper.early_stop}\")\n",
    "                return True\n",
    "\n",
    "        return (elbos, postfix_dict, pbar, early_stopping_callback, \n",
    "            test_errors, test_nll)\n",
    "            \n",
    "    else:\n",
    "        return elbos,postfix_dict,pbar,reporting_callback\n",
    "\n",
    "def get_bnn(model_params, inference, device=DEVICE, prior=None):\n",
    "    \"\"\"Builds a BNN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_params : dict\n",
    "    inference : str\n",
    "    device : str, optional\n",
    "        by default DEVICE\n",
    "    prior : tyxe.Prior, optional\n",
    "        If not given, the IID. Gaussian(0,1) is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tyxe.VariationalBNN\n",
    "    \"\"\"    \n",
    "    net = get_model(model_params).to(device)\n",
    "    obs = tyxe.likelihoods.Bernoulli(-1, event_dim=1, logit_predictions=False)\n",
    "\n",
    "    if inference == \"mean-field\":\n",
    "        \n",
    "        if prior is None:\n",
    "            prior_ = tyxe.priors.IIDPrior(dist.Normal(torch.tensor(\n",
    "                0., device=device), torch.tensor(1., device=device)),\n",
    "                expose_all=True)\n",
    "        else:\n",
    "            prior_ = prior\n",
    "        \n",
    "        guide_factory = functools.partial(\n",
    "            tyxe.guides.AutoNormal, init_scale=model_params[\"guide_init_scale\"],\n",
    "            init_loc_fn=tyxe.guides.PretrainedInitializer.from_net(net))\n",
    "    elif inference == \"ml\":\n",
    "        prior_ = tyxe.priors.IIDPrior(dist.Normal(0, 1), expose_all=False, hide_all=True)\n",
    "        guide_factory = None\n",
    "\n",
    "    return tyxe.VariationalBNN(net, prior_, obs, guide_factory)\n",
    "\n",
    "\n",
    "def load_bnn(model_params, inference, train_params, path):\n",
    "    \"\"\"Loads a BNN. Needs to call fit to instantiate a guide to then \n",
    "    load the weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_params : dict   \n",
    "    inference : str  \n",
    "    train_params : dict  \n",
    "    path : str or Path\n",
    "        for the state dict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tyxe.VariationalBNN\n",
    "    \"\"\"    \n",
    "    \n",
    "    # get bnn\n",
    "    new_bnn = get_bnn(model_params, inference)\n",
    "\n",
    "    # load data and create a single mini-batch dataloader\n",
    "    train_data = ASMGMovieLens(train_params[\"input_path\"], 1)\n",
    "    single_mbatch_data = random_split(train_data, [model_params[\n",
    "        \"batch_size\"], len(train_data) - model_params[\"batch_size\"]])[0]\n",
    "    singleton_loader = DataLoader(\n",
    "        single_mbatch_data, batch_size=model_params[\"batch_size\"])\n",
    "    new_bnn.likelihood.dataset_size = model_params[\"batch_size\"]\n",
    "\n",
    "    # define optimizer\n",
    "    optim = pyro.optim.Adam({\"lr\": model_params[\"learning_rate\"], \n",
    "    \"weight_decay\": model_params[\"l2_regularization_constant\"]})\n",
    "\n",
    "    # instance guide\n",
    "    with tyxe.poutine.local_reparameterization():\n",
    "        new_bnn.fit(singleton_loader, optim, 1, \n",
    "        device=DEVICE)\n",
    "    \n",
    "    # load BNN\n",
    "    new_bnn.load_state_dict(torch.load(path))\n",
    "\n",
    "    # prior is assumed to be the approximate posterior in case we\n",
    "    # are using mean field inference and prior update\n",
    "    if inference == \"mean-field\" and model_params[\"update_prior\"]:\n",
    "        new_bnn.update_prior(tyxe.priors.DictPrior(\n",
    "            new_bnn.net_guide.get_detached_distributions(\n",
    "            tyxe.util.pyro_sample_sites(new_bnn.net))))\n",
    "    return new_bnn\n",
    "\n",
    "\n",
    "\n",
    "def dataloader(\n",
    "    params, start_period, end_period=None, fast_dev_run=False, shuffle=True):\n",
    "    \"\"\"\n",
    "    Returns a dataloader and allows for fast development runs (testing).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : Namespace\n",
    "    start_period : int\n",
    "    end_period : int, by default None\n",
    "    fast_dev_run : bool, optional\n",
    "        by default False\n",
    "\n",
    "    \"\"\"\n",
    "    # restrict to just one minibatch for development testing\n",
    "    dataset = ASMGMovieLens(\n",
    "        params.input_path, start_period, end_period)\n",
    "\n",
    "    if fast_dev_run:\n",
    "        dataset = random_split(dataset, [model_params[\n",
    "        \"batch_size\"], len(dataset) - model_params[\"batch_size\"]])[0]\n",
    "\n",
    "    dataloader_ = DataLoader(\n",
    "        dataset, batch_size=params.batch_size, shuffle=shuffle,\n",
    "        num_workers=os.cpu_count(), pin_memory=USE_CUDA)\n",
    "            \n",
    "    return dataloader_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is no saved premodel\n",
    "if train_params[\"offline_path\"] is None:\n",
    "    offline_checkpoint_path = f\"{model_checkpoint_subdir}/offline_state_dict.pt\"\n",
    "    early_stopping = EarlyStopping(\n",
    "        delta=1e-4, path=offline_checkpoint_path, trace_func=lambda x: None) if \\\n",
    "            model_params[\"early_stopping_offline\"] else None\n",
    "    bnn = get_bnn(model_params, inference)\n",
    "    # ensure reproducibility\n",
    "    torch.manual_seed(train_params[\"seed\"])\n",
    "\n",
    "    # define periods\n",
    "    train_start_period = train_params[\"val_start_period\"] - train_params[\"train_window\"]\n",
    "    train_end_period = train_params[\"val_start_period\"] - 1\n",
    "    val_period = train_params[\"val_start_period\"]\n",
    "    print(\n",
    "        \"OFFLINE TRAINING\",\n",
    "        f\"train period: {train_start_period}-{train_end_period}\", \n",
    "        f\"validation period: {val_period}\", sep=\"\\n\")\n",
    "\n",
    "    # get dataloaders\n",
    "    train_loader = dataloader(params, train_start_period, train_end_period,\n",
    "        fast_dev_run=fast_dev_run)\n",
    "    test_loader = dataloader(params, val_period, fast_dev_run=fast_dev_run)\n",
    "\n",
    "    # initialize fit auxiliary variables\n",
    "    n_epochs_offline = model_params[\"n_epochs_offline\"] \n",
    "\n",
    "    if early_stopping is None:\n",
    "        elbos,postfix_dict, pbar, callback = fit_aux(\n",
    "            train_loader, n_epochs_offline, None)\n",
    "    else:\n",
    "        elbos,postfix_dict, pbar, callback, test_errors, test_nlls = fit_aux(\n",
    "            train_loader, n_epochs_offline, early_stopping, \n",
    "            test_dataloader=test_loader, test_samples=model_params[\"test_samples\"])\n",
    "\n",
    "    bnn.likelihood.dataset_size = len(train_loader.sampler)\n",
    "\n",
    "    optim = pyro.optim.Adam({\"lr\": model_params[\"learning_rate\"], \n",
    "        \"weight_decay\": model_params[\"l2_regularization_constant\"]})\n",
    "\n",
    "    with tyxe.poutine.local_reparameterization():\n",
    "        bnn.fit(train_loader, optim, n_epochs_offline, \n",
    "            device=DEVICE, callback=callback)\n",
    "\n",
    "\n",
    "    # if early stopping is None get validation performance, otherwise it is\n",
    "    # calculated on callback\n",
    "    if early_stopping is None:\n",
    "        mean_nll = validation_loop(bnn, test_loader, model_params[\"test_samples\"])[0]\n",
    "        postfix_dict[\"Val Loss\"] = f\"{mean_nll:.5f}\"\n",
    "        pbar.set_postfix(postfix_dict)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # counter starts from 1\n",
    "    print(f\"finished after {pbar.last_print_n} epochs\")\n",
    "\n",
    "    # update prior\n",
    "    if (inference == \"mean-field\") and params.update_prior:\n",
    "        bnn.update_prior(tyxe.priors.DictPrior(bnn.net_guide.get_detached_distributions(\n",
    "            tyxe.util.pyro_sample_sites(bnn.net))))\n",
    "\n",
    "\n",
    "    if early_stopping is None:\n",
    "        # save model \n",
    "        torch.save(bnn.state_dict(), offline_checkpoint_path)\n",
    "\n",
    "    else:\n",
    "        # load best model\n",
    "        bnn.load_state_dict(torch.load(early_stopping.path))\n",
    "\n",
    "else:\n",
    "    offline_checkpoint_path = params.offline_path\n",
    "    bnn = load_bnn(model_params, inference, train_params, params.offline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_offline:\n",
    "    # unit test for reproducibility of base model\n",
    "    torch.manual_seed(train_params[\"seed\"])\n",
    "    _, test_log_likelihood = torch.tensor([\n",
    "        bnn.evaluate(x.to(DEVICE), y.to(DEVICE), \n",
    "        num_predictions=model_params[\"test_samples\"]\n",
    "            ) for x, y in test_loader]\n",
    "        ).sum(dim = 0)\n",
    "    test_mean_nll = - test_log_likelihood.item() / len(test_loader.sampler)\n",
    "    print(f\"{test_mean_nll:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_offline:\n",
    "\n",
    "    # build new BNN\n",
    "    new_bnn = load_bnn(model_params, inference, train_params, offline_checkpoint_path)\n",
    "\n",
    "    torch.manual_seed(train_params[\"seed\"]) # if not set breaks\n",
    "    new_err, new_log_likelihood = torch.tensor([\n",
    "        new_bnn.evaluate(x.to(DEVICE), y.to(DEVICE), \n",
    "        num_predictions=model_params[\"test_samples\"]\n",
    "            ) for x, y in test_loader]\n",
    "        ).sum(dim = 0)\n",
    "    new_mean_nll = - new_log_likelihood.item() / len(test_loader.sampler)\n",
    "    print(f\"{new_mean_nll:.5f}\")\n",
    "    assert new_mean_nll == test_mean_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.save_result:\n",
    "\n",
    "    # save train performance statistics  \n",
    "    train_perf_path = f\"{model_checkpoint_subdir}/train_perf.csv\"\n",
    "    train_perf_dict = {\"elbo\": elbos}\n",
    "    if params.early_stopping_offline and (params.offline_path is None):\n",
    "        train_perf_dict.update({\n",
    "            \"test_error\": test_errors,\n",
    "            \"test_nlls\": test_nlls\n",
    "        })\n",
    "\n",
    "    train_perf_df = pd.DataFrame(train_perf_dict)\n",
    "    train_perf_df.index.set_names(\"epoch\", inplace=True)\n",
    "    train_perf_df.to_csv(train_perf_path)\n",
    "    print(f\"saving train performance statistics at: {os.path.abspath(train_perf_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_perf and (params.offline_path is None):\n",
    "    \n",
    "    # train losses\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.plot(elbos)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"ELBO loss\")\n",
    "    plt.title(\"Raw ELBO loss\")\n",
    "    plt.show()\n",
    "\n",
    "    if early_stopping is not None:\n",
    "        \n",
    "        # test errors\n",
    "        plt.figure(figsize=(9, 6))\n",
    "        plt.plot(test_errors)\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"test_error\")\n",
    "        plt.show()\n",
    "\n",
    "        # test losses\n",
    "        plt.figure(figsize=(9, 6))\n",
    "        plt.plot(test_nlls)\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"NLL\")\n",
    "\n",
    "        print(test_nlls[-1])\n",
    "        print(np.array(test_nlls).min(), np.array(test_nlls).argmin())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.online_end_of_validation_path is None:\n",
    "\n",
    "    # ensure reproducibility\n",
    "    torch.manual_seed(train_params[\"seed\"])\n",
    "\n",
    "    val_periods = range(\n",
    "        # starts at `val_start_period + 1` because the first validation is \n",
    "        # used for offline training\n",
    "        train_params[\"val_start_period\"] + 1, train_params[\"val_end_period\"] + 1)\n",
    "\n",
    "\n",
    "    n_epochs_online = model_params[\"n_epochs_online\"] \n",
    "\n",
    "    # initialize performance containers\n",
    "    val_losses = []\n",
    "    val_epochs = [] if params.early_stopping_online is not None else None\n",
    "    val_dict = defaultdict(lambda: [])\n",
    "\n",
    "    # same checkpoint path used along the online training\n",
    "    online_checkpoint_path = f\"{model_checkpoint_subdir}/online_state_dict.pt\"\n",
    "\n",
    "    for i, val_period in enumerate(val_periods, 1):\n",
    "\n",
    "        # initialize variational distribution randomly\n",
    "        if params.random_init:\n",
    "            new_bnn = get_bnn(model_params,inference)\n",
    "\n",
    "            # use last model approx. posterior as prior \n",
    "            if params.update_prior:\n",
    "                new_bnn.update_prior(tyxe.priors.DictPrior(\n",
    "                    bnn.net_guide.get_detached_distributions(\n",
    "                        tyxe.util.pyro_sample_sites(bnn.net))))\n",
    "                        \n",
    "            bnn = new_bnn\n",
    "            gc.collect()\n",
    "\n",
    "        # find the good number of epochs\n",
    "        early_stopping = EarlyStopping(\n",
    "            delta=1e-4, path=online_checkpoint_path, trace_func=lambda x: None) if \\\n",
    "                model_params[\"early_stopping_online\"] else None\n",
    "\n",
    "        # update periods\n",
    "        train_period = val_period - 1 \n",
    "        print(\n",
    "            f\"train period: {train_period}\", \n",
    "            f\"test period: {val_period}\", sep=\"\\n\")\n",
    "        \n",
    "\n",
    "        train_loader = dataloader(params, train_period,\n",
    "            fast_dev_run=fast_dev_run)\n",
    "        test_loader = dataloader(params, val_period, fast_dev_run=fast_dev_run, \n",
    "            shuffle=False)   \n",
    "\n",
    "        # initialize fit auxiliary variables\n",
    "        bnn.likelihood.dataset_size = len(train_loader.sampler)\n",
    "        \n",
    "\n",
    "\n",
    "        if early_stopping is None:\n",
    "            elbos,postfix_dict, pbar, callback = fit_aux(\n",
    "                train_loader, n_epochs_online, None)\n",
    "        else:\n",
    "            # find the optimal amount of epochs if early stopping is enabled\n",
    "            elbos,postfix_dict, pbar, callback, val_errors, val_nlls = fit_aux(\n",
    "                train_loader, n_epochs_online, early_stopping, \n",
    "                test_dataloader=test_loader, test_samples=model_params[\"test_samples\"])\n",
    "\n",
    "        bnn.likelihood.dataset_size = len(train_loader.sampler)\n",
    "\n",
    "        optim = pyro.optim.Adam({\"lr\": model_params[\"learning_rate\"], \n",
    "            \"weight_decay\": model_params[\"l2_regularization_constant\"]})\n",
    "\n",
    "        with tyxe.poutine.local_reparameterization():\n",
    "            bnn.fit(train_loader, optim, n_epochs_online, \n",
    "                device=DEVICE, callback=callback)\n",
    "\n",
    "        # if early stopping is None get validation performance, otherwise it is\n",
    "        # calculated on callback\n",
    "        if early_stopping is None:\n",
    "            mean_nll = validation_loop(bnn, test_loader, model_params[\"test_samples\"])[0]\n",
    "            postfix_dict[\"Val Loss\"] = f\"{mean_nll:.5f}\"\n",
    "            pbar.set_postfix(postfix_dict)\n",
    "            val_losses.append(mean_nll)\n",
    "        \n",
    "        else:\n",
    "            val_losses.append(early_stopping.best_score)\n",
    "            val_epochs.append(len(val_nlls) - (\n",
    "                early_stopping.patience * early_stopping.early_stop))\n",
    "            val_dict[f\"{val_period}-test_err\"] = val_errors\n",
    "            val_dict[f\"{val_period}-test_nll\"] = val_nlls\n",
    "\n",
    "        pbar.close()\n",
    "        print(f\"finished after {pbar.last_print_n} epochs\")\n",
    "\n",
    "        # val_losses.append(early_stopping.best_score)\n",
    "\n",
    "        if (inference == \"mean-field\") and params.update_prior:\n",
    "            bnn.update_prior(tyxe.priors.DictPrior(bnn.net_guide.get_detached_distributions(\n",
    "                tyxe.util.pyro_sample_sites(bnn.net))))\n",
    "        \n",
    "        # if early stopping is used, fallback to best model for next validation \n",
    "        # period\n",
    "        if early_stopping is not None:\n",
    "            # load best model\n",
    "            bnn.load_state_dict(torch.load(early_stopping.path))\n",
    "\n",
    "\n",
    "    # at the end of all validation periods, save the latest model in case \n",
    "    # not done by early stopping\n",
    "    if early_stopping is None:\n",
    "        # save model \n",
    "        torch.save(bnn.state_dict(), online_checkpoint_path)\n",
    "    else:\n",
    "        # save validation results which are deemed interesting only if early\n",
    "        # stopping is enabled\n",
    "        val_dict_path = f\"{model_checkpoint_subdir}/val_dict\"\n",
    "        save_as_json(val_dict, val_dict_path)\n",
    "\n",
    "else: \n",
    "    bnn = load_bnn(model_params, inference, train_params,\n",
    "        params.online_end_of_validation_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_online:\n",
    "\n",
    "    new_bnn = load_bnn(model_params, inference, train_params, offline_checkpoint_path)\n",
    "\n",
    "    # ensure reproducibility\n",
    "    torch.manual_seed(train_params[\"seed\"])\n",
    "\n",
    "    val_periods = range(\n",
    "    # starts at `val_start_period + 1` because the first validation is \n",
    "    # used for offline training\n",
    "    train_params[\"val_start_period\"] + 1, train_params[\"val_end_period\"] + 1)\n",
    "\n",
    "\n",
    "    n_epochs_online = model_params[\"n_epochs_online\"] \n",
    "\n",
    "    # initialize performance containers\n",
    "    val_losses = []\n",
    "    val_dict = defaultdict(lambda: [])\n",
    "\n",
    "    # same checkpoint path used along the online training\n",
    "    online_checkpoint_path = f\"{model_checkpoint_subdir}/test-reproducibility_online_state_dict.pt\"\n",
    "\n",
    "    for i, val_period in enumerate(val_periods, 1):\n",
    "\n",
    "        # find the good number of epochs\n",
    "        early_stopping = EarlyStopping(\n",
    "            delta=1e-4, path=online_checkpoint_path, trace_func=lambda x: None) if \\\n",
    "                model_params[\"early_stopping_online\"] else None\n",
    "\n",
    "        # update periods\n",
    "        train_period = val_period - 1 \n",
    "        print(\n",
    "            f\"train period: {train_period}\", \n",
    "            f\"test period: {val_period}\", sep=\"\\n\")\n",
    "        \n",
    "        train_loader = dataloader(params, train_period,\n",
    "            fast_dev_run=fast_dev_run)\n",
    "        test_loader = dataloader(params, val_period, fast_dev_run=fast_dev_run, \n",
    "            shuffle=False)   \n",
    "\n",
    "        # initialize fit auxiliary variables\n",
    "        new_bnn.likelihood.dataset_size = len(train_loader.sampler)\n",
    "\n",
    "\n",
    "        if early_stopping is None:\n",
    "            elbos,postfix_dict, pbar, callback = fit_aux(\n",
    "                train_loader, n_epochs_online, None)\n",
    "        else:\n",
    "            elbos,postfix_dict, pbar, callback, val_errors, val_nlls = fit_aux(\n",
    "                train_loader, n_epochs_online, early_stopping, \n",
    "                test_dataloader=test_loader, test_samples=model_params[\"test_samples\"])\n",
    "\n",
    "        new_bnn.likelihood.dataset_size = len(train_loader.sampler)\n",
    "\n",
    "        optim = pyro.optim.Adam({\"lr\": model_params[\"learning_rate\"], \n",
    "            \"weight_decay\": model_params[\"l2_regularization_constant\"]})\n",
    "\n",
    "        with tyxe.poutine.local_reparameterization():\n",
    "            new_bnn.fit(train_loader, optim, n_epochs_online, \n",
    "                device=DEVICE, callback=callback)\n",
    "\n",
    "\n",
    "        # if early stopping is None get validation performance, otherwise it is\n",
    "        # calculated on callback\n",
    "        if early_stopping is None:\n",
    "            mean_nll = validation_loop(new_bnn, test_loader, model_params[\"test_samples\"])[0]\n",
    "            postfix_dict[\"Val Loss\"] = f\"{mean_nll:.5f}\"\n",
    "            pbar.set_postfix(postfix_dict)\n",
    "            val_losses.append(mean_nll)\n",
    "        \n",
    "        else:\n",
    "            val_losses.append(early_stopping.best_score)\n",
    "            val_dict[f\"{val_period}-test_err\"] = val_errors\n",
    "            val_dict[f\"{val_period}-test_nll\"] = val_nlls\n",
    "\n",
    "        pbar.close()\n",
    "        print(f\"finished after {pbar.last_print_n} epochs\")\n",
    "\n",
    "        if (inference == \"mean-field\") and params.update_prior:\n",
    "            new_bnn.update_prior(tyxe.priors.DictPrior(new_bnn.net_guide.get_detached_distributions(\n",
    "                tyxe.util.pyro_sample_sites(new_bnn.net))))\n",
    "        \n",
    "        # if early stopping is used, fallback to best model for next validation \n",
    "        # period\n",
    "        if early_stopping is not None:\n",
    "            # load best model\n",
    "            new_bnn.load_state_dict(torch.load(early_stopping.path))\n",
    "\n",
    "\n",
    "    # at the end of all validation periods, save the latest model in case \n",
    "    # not done by early stopping\n",
    "    if early_stopping is None:\n",
    "        # save model \n",
    "        torch.save(new_bnn.state_dict(), online_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.early_stopping_online:\n",
    "    # select the number of epochs for online test from validation\n",
    "    n_epochs_online = int(pd.Series(val_epochs).median())\n",
    "    print(f\"optimal online epochs: {n_epochs_online}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if train_params[\"test_start_period\"] is not None:\n",
    "    test_periods = range(\n",
    "        train_params[\"test_start_period\"], train_params[\"test_end_period\"] + 1)\n",
    "else:\n",
    "    test_periods = []\n",
    "    print(\"skipped test cycle\")\n",
    "\n",
    "\n",
    "\n",
    "test_dict = defaultdict(lambda: [])\n",
    "\n",
    "for i, test_period in enumerate(test_periods, 1):\n",
    "\n",
    "    # initialize variational distribution randomly\n",
    "    if params.random_init:\n",
    "        new_bnn = get_bnn(model_params,inference)\n",
    "\n",
    "        # use last model approx. posterior as prior \n",
    "        if params.update_prior:\n",
    "            new_bnn.update_prior(tyxe.priors.DictPrior(\n",
    "                bnn.net_guide.get_detached_distributions(\n",
    "                    tyxe.util.pyro_sample_sites(bnn.net))))\n",
    "                    \n",
    "        bnn = new_bnn\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    # update periods\n",
    "    train_period = test_period - 1 \n",
    "    print(\n",
    "        f\"train period: {train_period}\", \n",
    "        f\"test period: {test_period}\", sep=\"\\n\")\n",
    "\n",
    "    train_loader = dataloader(params, train_period, \n",
    "        fast_dev_run=fast_dev_run)\n",
    "    test_loader = dataloader(params, test_period, fast_dev_run=fast_dev_run, \n",
    "        shuffle=False)   \n",
    "\n",
    "    elbos,postfix_dict, pbar, callback = fit_aux(\n",
    "                train_loader, n_epochs_online, None)\n",
    "\n",
    "    bnn.likelihood.dataset_size = len(train_loader.sampler)\n",
    "    optim = pyro.optim.Adam({\"lr\": model_params[\"learning_rate\"], \n",
    "    \"weight_decay\": model_params[\"l2_regularization_constant\"]})\n",
    "\n",
    "\n",
    "    # for epoch in range(n_epochs_online):\n",
    "\n",
    "    with tyxe.poutine.local_reparameterization():\n",
    "        bnn.fit(train_loader, optim, n_epochs_online, \n",
    "            device=DEVICE, callback=callback)\n",
    "\n",
    "    mean_nll, mean_error = validation_loop(\n",
    "        bnn, test_loader, model_params[\"test_samples\"])\n",
    "    postfix_dict[\"Val Loss\"] = f\"{mean_nll:.5f}\"\n",
    "    pbar.set_postfix(postfix_dict)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    test_dict[\"period\"].append(test_period)\n",
    "    test_dict[\"loss\"].append(mean_nll)\n",
    "    test_dict[\"error\"].append(mean_error)\n",
    "\n",
    "    if (inference == \"mean-field\") and params.update_prior:\n",
    "        bnn.update_prior(tyxe.priors.DictPrior(bnn.net_guide.get_detached_distributions(\n",
    "            tyxe.util.pyro_sample_sites(bnn.net))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = f\"{model_checkpoint_subdir}/first_biu.csv\"\n",
    "res_df = pd.DataFrame(test_dict)\n",
    "average_srs = res_df.mean()\n",
    "average_srs.at[\"period\"] = \"mean\"\n",
    "print(average_srs)\n",
    "\n",
    "if train_params[\"save_result\"]: \n",
    "    pd.concat((res_df, average_srs.to_frame().T), axis=0, ignore_index=True\n",
    "    ).to_csv(df_path, index=False)\n",
    "    print(f\"saved results csv at: {os.path.abspath(df_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn.prior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('tyxe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e8b746a8311f1ca0a88d9ac835a0b13cb2eeabaa3ffb77d4d6963e5660a652f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
